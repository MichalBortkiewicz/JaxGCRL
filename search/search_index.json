{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"JaxGCRL","text":"<p>JaxGCRL is a high-performance library and benchmark for self-supervised goal-conditioned reinforcement learning.  Leveraging efficient GPU acceleration, the framework enables researchers to train agents for millions of environment  steps within minutes on a single GPU.</p> <ul> <li>Blazing Fast Training - Train 10 million environment steps in 10    minutes on a single GPU, up to 22\\(\\times\\) faster than prior implementations.</li> <li>Comprehensive Benchmarking - Includes 10+ diverse environments and multiple pre-implemented baselines for out-of-the-box evaluation.</li> <li>Modular Implementation - Designed for clarity and scalability,    allowing for easy modification of algorithms.</li> </ul>"},{"location":"#structure-of-the-code","title":"Structure of the Code","text":"<p>The codebase is organized into several key files and directories. Below is an overview of the structure and most important files:</p> <pre><code>\n\u251c\u2500\u2500 src: Algorithm code (training, network, replay buffer, etc.)\n\u2502   \u251c\u2500\u2500 train.py: Main file. Defines energy functions + losses, and networks. Collects trajectories, trains networks, runs evaluations.\n\u2502   \u251c\u2500\u2500 replay_buffer.py: Contains replay buffer, including logic for state, action, and goal sampling for training.\n\u2502   \u2514\u2500\u2500 evaluator.py: Runs evaluation and collects metrics.\n\u251c\u2500\u2500 envs: Environments (python files and XMLs)\n\u2502   \u251c\u2500\u2500 ant.py, humanoid.py, ...: Most environments are here\n\u2502   \u251c\u2500\u2500 assets: Contains XMLs for environments\n\u2502   \u2514\u2500\u2500 manipulation: Contains all manipulation environments\n\u251c\u2500\u2500 scripts/train.sh: Modify to choose environment and hyperparameters\n\u251c\u2500\u2500 utils.py: Logic for script argument processing, rendering, environment names, etc.\n\u2514\u2500\u2500 training.py: Interface file that processes script arguments, calls train.py, initializes wandb, etc.\n</code></pre>"},{"location":"#paper-accelerating-goal-conditioned-rl-algorithms-and-research","title":"Paper: Accelerating Goal-Conditioned RL Algorithms and Research","text":"<p> Training CRL on Ant environment for 10M steps takes only ~10 minutes on 1 Nvidia V100.  </p> <p>Abstract: Self-supervision has the potential to transform reinforcement learning (RL), paralleling the breakthroughs it has enabled in other areas of machine learning. While self-supervised learning in other domains aims to find patterns in a fixed dataset, self-supervised goal-conditioned reinforcement learning (GCRL) agents discover new behaviors by learning from the goals achieved during unstructured interaction with the environment. However, these methods have failed to see similar success, both due to a lack of data from slow environment simulations as well as a lack of stable algorithms. We take a step toward addressing both of these issues by releasing a high-performance codebase and benchmark (JaxGCRL) for self-supervised GCRL, enabling researchers to train agents for millions of environment steps in minutes on a single GPU. By utilizing GPU-accelerated replay buffers, environments, and a stable contrastive RL algorithm, we reduce training time by up to \\(\\mathbf{22\\times}\\) . Additionally, we assess key design choices in contrastive RL, identifying those that most effectively stabilize and enhance training performance. With this approach, we provide a foundation for future research in self-supervised GCRL, enabling researchers to quickly iterate on new ideas and evaluate them in diverse and challenging environments.</p>"},{"location":"environments/","title":"Environments","text":"<p>We provide 8 blazingly fast goal-conditioned environments based on MJX and BRAX and jitted framework for  quick experimentation with goal-conditioned self-supervised reinforcement learning.</p> Environment Env name Code Reacher <code>reacher</code> link Half Cheetah <code>cheetah</code> link Pusher <code>pusher_easy</code> <code>pusher_hard</code> link Ant <code>ant</code> link Ant Maze <code>ant_u_maze</code> <code>ant_big_maze</code> <code>ant_hardest_maze</code> link Ant Soccer <code>ant_ball</code> link Ant Push <code>ant_push</code> link Humanoid <code>humanoid</code> link"},{"location":"environments/#adding-new-environments","title":"Adding new environments","text":"<p>Each environment implementation has 2 main parts: an XML file and a Python file. </p> <p>The XML file contains information about geometries, placements, properties, and movements of objects in the environment. Depending on the Brax pipeline used, the XML file may vary slightly, but generally, it should follow MuJoCo XML reference. Since all environments are vectorized and compiled with JAX, the information in MJX guide should also be taken into consideration, particularly the feature parity section and performance tuning section.</p> <p>XML files</p> <p>In our experience XML files that worked with standard MuJoCo require some tuning for MJX. In particular, the number of solver iterations should be carefully adjusted, so that the environment is fast but still stable.</p> <p>The Python file contains the logic of the environment, a description of how the environment is initialized, restored, and how one environment step looks. The class describing the environment should inherit from BRAX's <code>PipelineEnv</code> class. All environment logic should be JIT-able with JAX, which requires some care in using certain Python instructions like <code>if</code> and <code>for</code>. The observation returned by the <code>step</code> function of the environment should be a state of the environment concatenated with the current environment goal. Each environment class should also provide 2 additional properties: * <code>self.state_dim</code> - The size of the state of the environment (that is observation without the goal). * <code>self.goal_indices</code> - Array with state indices that make the goal. For example, in the <code>Ant</code> environment the goal is specified as the x and y coordinates of the torso. Thus we specify <code>self.goal_indices = jnp.array([0, 1])</code>, since the x and y coordinates of the torso are at positions 0 and 1 in the state of the environment.</p> <p>To use the new environment it should be added to the <code>create_env</code> function in <code>utils.py</code>.</p>"},{"location":"install/","title":"Installation","text":""},{"location":"install/#environment-creation","title":"Environment creation","text":"<p>The entire process of installing the benchmark is just one step using the conda <code>environment.yml</code> file. <pre><code>conda env create -f environment.yml\n</code></pre></p> <p>To check whether installation worked, run a test experiment using <code>./scripts/train.sh</code> file:</p> <pre><code>chmod +x ./scripts/train.sh; ./scripts/train.sh\n</code></pre>"},{"location":"methods/","title":"Methods","text":""},{"location":"methods/#contrastive-rl","title":"Contrastive RL","text":"<p>The main algorithm implemented in this repository is Contrastive Reinforcement Learning (CRL), first described in a paper Contrastive Learning as Goal-Conditioned Reinforcement Learning. This Algorithm learns a critic function, without rewards, by contrasting states sampled from a future of a given trajectory (positive samples) with states sampled from a random trajectory (negative samples).  We implemented a number of modifications and improvements to this algorithm. Among those, the most important ones are:</p> <ul> <li>Choice of the energy function, currently we have implemented: <code>l1</code>, <code>l2</code>, <code>l2_no_sqrt</code>, <code>dot</code>, and <code>cos</code> energy functions.</li> <li>Choice of the contrastive loss, currently we have implemented: <code>binary</code>, <code>infonce</code>, <code>infonce_backward</code>, <code>symmetric_infonce</code>, <code>flat_infonce</code>, <code>flat_infonce_backward</code>, <code>forward_backward</code>, <code>dpo</code>. <code>ipo</code>, and <code>sppo</code> losses.</li> <li><code>logsumex_penalty</code>- this is a regularizing term applied to contrastive loss, that we have found improves training in most settings.</li> </ul> <p>All of the above, and more, can be found and modified  in <code>src/losses.py</code> file.</p>"},{"location":"methods/#other-baselines","title":"Other baselines","text":"<p>To easily compare CRL to other well-known RL algorithms we have implemented several other baselines including PPO, SAC, and TD3, based on implementations in Brax. For SAC and TD3 it is additionally possible to enable <code>--use_her</code> flag to take advantage of the Hindsight Experience Replay buffer, which can improve performance in sparse reward setting. </p> <p>While reward shaping was not our main priority, since CRL algorithm doesn't use reward, most environments can provide both sparse and dense rewards, by using <code>--use_dense_reward</code> flag.</p>"},{"location":"methods/#adding-new-methods-and-algorithms","title":"Adding new methods and algorithms","text":"<p>The primary purpose of our work is to enable easy and rapid research on Self-Supervised Goal-Conditioned Reinforcement Learning. Thus adding new losses, and energy functions, or changing other aspects of the algorithm can be easily done, by modifying <code>src/losses.py</code> and/or <code>src/networks.py</code>, which are easily readable and accessible.</p>"},{"location":"methods/#adding-new-contrastive-objective","title":"Adding new contrastive objective","text":"<p>For instance, to register a new contrastive objective (\"<code>your_loss</code>\") for CRL, you need to: 1. Register new <code>contrastive_loss_fn</code> in <code>crl_critic_loss</code> function (<code>src/losses.py</code> file): <pre><code>...\nif contrastive_loss_fn == \"your_loss\":\n    loss = ...\n...\n</code></pre> 2. Run training with your new contrastive objective, to check if algorithm learns properly: <pre><code>python training.py --contrastive_loss_fn \"your_loss\"\n</code></pre></p>"},{"location":"methods/#using-a-custom-model-architecture","title":"Using a custom model architecture.","text":"<p>To integrate a custom model architecture for CRL algorithm, you need to define and register your architecture within the <code>src/cnetworks.py</code> file:  1. Critic: extend or modify the <code>MLP</code> class or create a new model used for contrastive embeddings.  2. Actor: provide appropriate <code>make_policy_network</code> function that defines actor architecture.    </p> <p>Algorithms, that differ from CRL (or one of the other implemented baselines) in a more fundamental way (e.g. non-standard replay buffer, not relaying on actor and critic as a main paradigm) can also be implemented, but will usually require modification of <code>src/train.py</code>, which requires some technical knowledge on JAX, especially how JIT mechanism works.</p>"},{"location":"usage/","title":"Basic Usage","text":""},{"location":"usage/#experiments","title":"Experiments","text":"<p>JaxGCRL is highly flexible in terms of parameters, allowing for a wide range of experimental setups. To run a basic experiment, you can start with: <pre><code>python training.py --env_name ant\n</code></pre> For a complete list of environments, refer to the environments section or source code.</p>"},{"location":"usage/#number-of-environments","title":"Number of environments","text":"<p>One of JaxGCRL's key features is its ability to run parallel environments for data collection. If your GPU has limited memory, you can reduce the number of parallel environments with the following parameter:</p> <pre><code>python training.py --env_name ant --num_envs 16 --batch_size 16\n</code></pre> <p>Replay buffer</p> <p><code>num_envs * (episode_length - 1)</code> must be divisible by <code>batch_size</code> due to the way data is stored in replay buffer.</p>"},{"location":"usage/#training-hyperparameters","title":"Training hyperparameters","text":"<p>You can customize the neural network architecture by adjusting the number of hidden layers (<code>n_hidden</code>), the width of hidden layers (<code>h_dim</code>) and the representation dimension (<code>repr_dim</code>): <pre><code>python training.py --env_name ant --h_dim 128 --n_hidden 3 --repr_dim 32\n</code></pre></p> <p>JaxGCRL supports various built-in energy functions and contrastive loss functions. For example, to use an L2 penalty with the InfoNCE-backward contrastive loss, you can run: <pre><code>python training.py --env_name ant --energy_fn l2 --contrastive_loss_fn infonce_backward\n</code></pre> For a full list of available energy functions and contrastive losses, see: [ energy functions | contrastive losses ]</p> <p>JaxGCRL offers many other useful parameters, such as <code>num_timesteps</code>, <code>batch_size</code>, <code>episode_length</code>. For a complete list of parameters, their descriptions, and default values, refer to link.</p> <p>You can also find additional information about main arguments and hyperparameters by running:</p> <p><code>python training.py --help</code></p> <p>To execute multiple experiments, you can use a bash script. For example, the script below runs experiments to test the performance of different contrastive losses: <pre><code>for c_loss in binary infonce flatnce fb dp; do\n    for seed in 1 2 3 4 5; do\n        python training.py --seed ${seed} \\\n        --project_name crl --group_name contrastive_loss_experiments \\ \n        --exp_name ${c_loss} \\\n        --contrastive_loss_fn ${c_loss} --energy_fn l2 \\\n        --log_wandb\n    done\ndone\n</code></pre></p>"},{"location":"usage/#wandb-support","title":"Wandb support","text":"<p>Wandb account</p> <p>If you haven't configured yet <code>wandb</code>, you might be prompted to log in.</p> <p>We highly recommend using Wandb for tracking and visualizing your results (Wandb support). Enable Wandb logging with the <code>--log_wandb</code> flag. Additionally, you can organize experiments with the following flags: - <code>--project_name</code> - <code>--group_name</code> - <code>--exp_name</code></p> <p>All of the metric runs are logged into <code>wandb</code>. We recommend using it as a tool for running sweep over hyperparameters.</p> <ol> <li>Run exemplary <code>sweep</code>: <pre><code>wandb sweep --project exemplary_sweep ./scripts/sweep.yml\n</code></pre></li> <li>Then run wandb agent with : <pre><code>wandb agent &lt;previous_command_output&gt;\n</code></pre></li> </ol> <p>Besides logging the metrics, we also render final policy to <code>wandb</code> artifacts. </p> <p> </p>"}]}