program: training.py
method: bayes
metric:
  goal: maximize
  name: eval/episode_reward
parameters:
  policy_lr:
    values:
      - 1e-4
      - 1e-3
      - 1e-2
    distribution: categorical
  alpha_lr:
    values:
      - 1e-5
      - 1e-4
      - 1e-3
      - 1e-2
    distribution: categorical
  critic_lr:
    values:
      - 1e-5
      - 1e-4
      - 1e-3
      - 1e-2
    distribution: categorical
  discounting:
    min: 0.95
    max: 0.99
    distribution: uniform
  batch_size:
    min: 128
    max: 1024
    distribution: int_uniform
  contrastive_loss_fn:
    values:
      - "symmetric_infonce"
      - "binary"
      - "infonce"
      - "infonce_backward"
    distribution: categorical
  logsumexp_penalty:
    values:
      - 0.0
      - 0.01
    distribution: categorical
  seed:
    min: 0
    max: 100
    distribution: int_uniform
  env_name:
    value: "ant"
    distribution: constant
command:
  - ${env}
  - python
  - ${program}
  - --log_wandb
  - --use_tested_args
  - ${args_no_boolean_flags}

